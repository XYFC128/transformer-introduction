# Transformer Introduction

Here are some nicely drawn diagrams to help you *actually* understand self-attention and transformer. Full report and presentation slide can be downloaded from the [Release](https://github.com/XYFC128/transformer-introduction/releases) page.

## Self-Attention

### QKV projection

![Query Projection](figures/query.webp)

![Key Projection](figures/key.webp)

![Value Projection](figures/value.webp)

### QK Transpose (Dot-Product)

![QK Transpose](figures/qk-transpose.webp)

### Softmax

![Softmax](figures/softmax.webp)

### Single-Head Attention

![Single-Head Attention](figures/single-head-attention.webp)

### Multi-Head Attention

![Simplified Multi-Head Attention](figures/multihead-attention-simple.webp)

![Multi-Head Attention](figures/multihead-attention-actual.webp)

### Masked Attention

![Masked Attention](figures/masked-attention.webp)

### Cross Attention

![Cross Attention](figures/cross-attention.webp)

## Transformer

![Transformer](figures/transformer.webp)

## License

The code for the slide and the report is licensed under the [MIT](https://opensource.org/license/mit) license. The figures are licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.
